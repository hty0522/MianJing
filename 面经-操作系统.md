# 操作系统

### 小点：

什么是 “可重入”，可重入就是说某个线程已经获得某个锁，可以再次获取锁而不会出现死锁





### ------------------------------

### IO多路复用

​		IO多路复用是一种**同步IO模型**，实现一个线程可以监视多个文件句柄；一旦某个文件句柄就绪，就能够通知应用程序进行相应的读写操作；没有文件句柄就绪时会阻塞应用程序，交出cpu。多路是指网络连接，复用指的是同一个线程

​		没有IO多路复用机制时，有**同步阻塞**和**同步非阻塞**

​		**同步阻塞（BIO）**服务端采用单线程，当accept一个请求后，在recv或send调用阻塞时，将无法accept其他请求（必须等上一个请求处recv或send完），`无法处理并发`

![image-20220112205543013](面经-操作系统.assets\image-20220112205543013.png)

​		服务器端采用多线程，当accept一个请求后，开启线程进行recv，可以完成并发处理，但随着请求数增加需要增加系统线程，`大量的线程占用很大的内存空间，并且线程切换会带来很大的开销，10000个线程真正发生读写事件的线程数不会超过20%，每次accept都开一个线程也是一种资源浪费`

​		**同步非阻塞** ：服务器端当accept一个请求后，加入fds集合，每次轮询一遍fds集合recv(非阻塞)数据，没有数据则立即返回错误，`每次轮询所有fd（包括没有发生读写事件的fd）会很浪费cpu`

![image-20220112205643246](面经-操作系统.assets\image-20220112205643246.png)

#### IO多路复用（现在的做法）

服务器端采用单线程通过select/epoll等系统调用获取fd列表，遍历有事件的fd进行accept/recv/send，使其能`支持更多的并发连接请求`

fd指文件描述符

- ​	select

- ​	poll 与select相比，只是没有fd的限制

- ​	epoll

  ![image-20220112205621975](面经-操作系统.assets\image-20220112205621975.png)

#### **select缺点**



 I/O多路复用支持只使用一个进程来维护多个 Socket。I/O 多路复用技术会用一个系统调用函数来监听我们所有关心的连接，也就说可以在一个监控线程里面监控很多的连接。只有当连接上有数据的时候，线程才去发起读请求。

        select/poll/epoll 就是内核提供给用户态的多路复用系统调用，线程可以通过一个系统调用函数从内核中获取多个事件。


​        select实现多路复用的方式是，将已连接的 Socket 都放到一个文件描述符集合，然后调用 select 函数将文件描述符集合拷贝到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过遍历文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合拷贝回用户态里，然后用户态还需要再通过遍历的方法找到可读或可写的 Socket，然后再对其处理。

        所以，对于 select 这种方式，需要进行 2 次「遍历」文件描述符集合，一次是在内核态里，一个次是在用户态里 ，而且还会发生 2 次「拷贝」文件描述符集合，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。
    
        select 使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最大值为 1024，只能监听 0~1023 的文件描述符。
​		**单个进程所打开的FD是有限制的**，通过FD_SETSIZE设置，默认1024

​		每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大

​		对socket扫描时是线性扫描，采用轮询的方法，效率较低（高并发时）

#### epoll缺点

​		只能工作在Linux下

#### **epoll LT 与 ET模式的区别**

- ​		epoll有EPOLLLT和EPOLLET两种触发模式，LT是默认的模式，ET是“高速”模式。
- ​        LT模式下，只要这个fd还有数据可读，每次 epoll_wait都会返回它的事件，提醒用户程序去操作
- ​        ET模式下，它只会提示一次，直到下次再有数据流入之前都不会再提示了，无论fd中是否还有数据可读。所以在ET模式下，read一个fd的时候一定要把它的buffer读完，或者遇到EAGAIN错误

![image-20220112195058689](面经-操作系统.assets\image-20220112195058689.png)

### ------------------------------

### 为什么要设置虚拟内存

内存和cpu一样都是稀缺资源，两个程序不能在内存重叠的情况下一起运行，因此借助虚拟内存将进程和真实的物理内存连接起来，进程持有的虚拟地址通过内存管理单元（MMU）的映射关系，来转变为物理地址，再通过物理地址访问内存。

内存分段和内存分页都是操作虚拟地址和物理地址之间关系的方式，通过段号查询段表得到段基地址，再加上偏移量就是实际地址，但存在内存碎片和内存交换效率低的问题

内存分页是将虚拟和物理内存分割成一段段固定尺寸的大小（4KB），最小操作单位小，因此内存碎片和交换效率都有所改善。

根据页号查页表得到物理页号+偏移量  》〉》 页表数量太多，存储页表开销太大

因此有了多级页表，解决存储页表开销大的问题

段式与页式并不是相对的，他们也可以组合在一起使用，在段的基础上进行分页分级，虚拟地址结构由**段号、段内页号和页内位移**三部分组成

TBL缓存，把最常用到的页表放到TBL缓存中去，命中了就直接得到物理地址，中不了再多级页表查找。



### 什么是操作系统内存

​		运行程序之前向操作系统申请内存，然后运行程序，一段时间后操作系统会来整理内存空间碎片，电脑上的程序运行是需要对应大小的物理内存。

#### 虚拟内存

​		实际上运行的进程并不是直接使用物理内存地址，而是把进程使用的内存地址与实际的物理内存地址做隔离，即操作系统会为每个进程分配独立的一套「**虚拟地址**」。

​		每个进程玩自己的地址，互不干涉，至于虚拟地址怎么映射到物理地址，对进程来说是透明的，操作系统已经把这些安排的明明白白了。操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来，如下图所示

- ​		进程中使用的内存地址叫虚拟地址
- ​		存在计算硬件里的空间地址叫物理地址

#### 操作系统是如何管理虚拟地址与物理内存地址之间关系?

#### 内存分段

​		程序包含若干个逻辑分段，如可由代码段、数据段、栈段、堆段组成，每个分段都有不同的属性，所以内存以分段的形式把这些段分离出来进行管理。

​		分段管理下的虚拟地址由两部分组成，**段号和段内偏移量**

​		1.通过段号映射段表的项

​		2.从项中获取到段基地址

​		3.段基地址+段内偏移量=使用的物理内存



![s](面经-操作系统.assets\image-20220112200651850.png)

两个不足：

- 内存碎片的问题
- 内存交换的效率低的问题

​		内存碎片整理通过内存交换的方式来实现，将内存上的程序加载到硬盘上去，再从硬盘读回来，内存交换空间，在 Linux 系统里，是我们常看到的 **Swap 空间**，这块空间是从硬盘划分出来的，用于内存与硬盘的空间交换。



​		首先分段管理容易造成内存碎片，导致内存交换的频率较高，因为**硬盘的访问速度比内存慢太多了**，然后每次交换的时候，把一大段连续的内存写入到硬盘，再又从硬盘读取出来，如果交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿，过程也很慢的，所以说**分段方式内存交换效率低**。

#### 内存分页

​		内存碎片是由**多个不连续的小物理内存空间**造成，如果把这些不连续的小物理内存空间组合起来，是不是解决了这个问题？同样的，内存交换的时候我们保证交换的数据小，是不是能提高内存交换的效率？

​		这个办法就是内存分页，分页是把整个虚拟与物理空间切成一段段固定尺寸的大小，这样一个连续并且尺寸固定的空间，我们叫页，在 Linux 下，**每一页的大小为 4KB**。（虚拟空间是指存储一套虚拟地址的空间）

​		虚拟地址与物理地址是**通过页表来映射**，虚拟空间内的虚拟地址一定是连续的，物理地址不一定，但可以通过连续的虚拟地址把多个不连续的物理内存组合使用。



![image-20220112202733253](面经-操作系统.assets\image-20220112202733253.png)

1. 页号找到页表中的页项
2. 获取页项的物理页号基地址
3. 偏移量+物理页号基地址计算出物理内存地址

![image-20220112202903512](面经-操作系统.assets\image-20220112202903512.png)

每个进程分配一个页表，因为操作系统有非常多的进程，因此页表数量会很多，通过多级页表，可以节约内存

![image-20220112203347047](面经-操作系统.assets\image-20220112203347047.png)

#### TBL

​		多级页表虽然解决了空间上的问题，但是我们发现这种方式需要走多道转换才能找到映射的物理内存地址，经过的**多道转换造成了时间上的开销**。操作系统就利用这一特性，**把最多使用的几个页表项放到TBL缓存**, CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的页表，TLB 的命中率其实很高的，因为程序最常访问的页就那么几个。

#### 内存段页

​		段式与页式并不是相对的，他们也可以组合在一起使用，在段的基础上进行分页分级

1. 先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制 
2. 接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页 

​		虚拟地址结构由**段号、段内页号和页内位移**三部分组成

![image-20220112203548746](面经-操作系统.assets\image-20220112203548746.png)

1. 通过段号获取段表的段项
2. 通过段项获取到页表地址
3. 通过页表地址找到段页表
4. 通过段内页号找到段页表的段页项
5. 通过段页项获取物理页基地址
6. 通过物理页基地址+偏移量计算出物理内存地址

### ------------------------------

### 用户态与内核态

​		一般的操作系统对执行权限进行分级，分别为用保护态和内核态。用户态相较于内核态有**较低的执行权限**，很多操作是不被操作系统允许的，原因简单来说就是**用户态出现问题，也不能让操作系统崩溃呀。**		

​		内核态相当于一个介于硬件与应用之间的层，内核有ring 0的权限，可以执行任何cpu指令，也可以引用任何内存地址，包括外围设备, 例如硬盘, 网卡，权限等级最高。

#### 状态转换

​		用户程序跑在用户态下，但是如果需要执行一些操作例如**申请内存，网络读写**时，自己的权限不够，就需要转换到内核态去让内核的code帮忙干一些事情。下面三个方式是

######  a. 系统调用

​		这是用户态进程主动要求切换到内核态的一种方式，用户态进程通过系统调用申请使 用操作系统提供的服务程序完成工作，比如前例中fork()实际上就是执行了一个创建新进程的系统调用。而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的int 80h中断。

######  b. 异常

​		当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如**缺页异常**。

######  c. 外围设备的中断

​		当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会 暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序，如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到 内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。

#### 为什么两者切换耗时

​		linux下每个进程的栈有两个，一个是**用户态栈，一个是内核态栈**。在需要从用户态栈切换到内核的时候，需要进行执行栈的转换，保存用户态的状态，包括寄存器状态，然后执行内核态操作，操作完成后要**恢复现场**，切换到用户态，这个过程是**耗时**的。